{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "To understand anything that's going on below, check first the slides / video lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nn_Softargmax = nn.Softmax  # fix wrong name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_input=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        if d_input is None:\n",
    "            d_xq = d_xk = d_xv = d_model\n",
    "        else:\n",
    "            d_xq, d_xk, d_xv = d_input\n",
    "            \n",
    "        # Make sure that the embedding dimension of the model is a multiple of\n",
    "        # a number of heads\n",
    "        assert d_model % self.num_heads == 0\n",
    "        # length of query and key should match\n",
    "        assert d_xq == d_xk\n",
    "        self.d_k = d_model // self.num_heads\n",
    "        \n",
    "        # These are still of dimension d_model. They will be split into number\n",
    "        # of heads \n",
    "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
    "        \n",
    "        # Outputs of all sub-layers need to be of dimension d_model\n",
    "        self.W_h = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        # batch_size = Q.size(0) \n",
    "        # k_length = dim per head\n",
    "        # k_length = K.size(-2) \n",
    "        \n",
    "        # Scaling by d_k so that the soft(arg)max doesnt saturate\n",
    "        # (bs, n_heads, q_length, k_length)\n",
    "        Q = Q / np.sqrt(self.d_k)\n",
    "        # (bs, n_heads, q_length, k_length)\n",
    "        scores = torch.matmul(Q, K.transpose(2, 3))        \n",
    "        # (bs, n_heads, q_length, k_length)\n",
    "        A = nn_Softargmax(dim=-1)(scores)\n",
    "        \n",
    "        # Get the weighted average of the values\n",
    "        # (bs, n_heads, q_length, k_length)\n",
    "        H = torch.matmul(A, V)\n",
    "\n",
    "        return H, A \n",
    "\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Split the last dimension (embedding dim) into (heads X depth)\n",
    "        Return after transpose to put in shape\n",
    "        (batch_size X num_heads X seq_length X d_k)\n",
    "        \"\"\"\n",
    "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def group_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Combine the heads again to get\n",
    "        (batch_size X seq_length X (num_heads * d_k))\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.num_heads * self.d_k)\n",
    "    \n",
    "\n",
    "    def forward(self, X_q, X_k, X_v):\n",
    "        batch_size, _, _ = X_q.size()\n",
    "\n",
    "        # After transforming, split into num_heads \n",
    "        # (bs, n_heads, q_length, dim_per_head)\n",
    "        Q = self.split_heads(self.W_q(X_q), batch_size)\n",
    "        # (bs, n_heads, k_length, dim_per_head)\n",
    "        K = self.split_heads(self.W_k(X_k), batch_size)\n",
    "        # (bs, n_heads, v_length, dim_per_head)\n",
    "        V = self.split_heads(self.W_v(X_v), batch_size)\n",
    "        # Calculate the attention weights for each of the heads\n",
    "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        # Put all the heads back together by concat\n",
    "        # (bs, q_length, dim)\n",
    "        H_cat = self.group_heads(H_cat, batch_size)\n",
    "        \n",
    "        # Final linear layer\n",
    "        # (bs, q_length, dim)\n",
    "        H = self.W_h(H_cat)\n",
    "\n",
    "        return H, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "def print_out(Q, K, V):\n",
    "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
    "    print('Attention weights are:', temp_attn.squeeze())\n",
    "    print('Output is:', temp_out.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our self attention works - if the query matches with one of the key values, it should have all the attention focused there, with the value returned as the value at that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
      "Output is: tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "test_K = torch.tensor(\n",
    "    [[10, 0, 0],\n",
    "     [ 0,10, 0],\n",
    "     [ 0, 0,10],\n",
    "     [ 0, 0,10]]\n",
    ").float()[None,None]\n",
    "\n",
    "test_V = torch.tensor(\n",
    "    [[   1,0,0],\n",
    "     [  10,0,0],\n",
    "     [ 100,5,0],\n",
    "     [1000,6,0]]\n",
    ").float()[None,None]\n",
    "\n",
    "test_Q = torch.tensor(\n",
    "    [[0, 10, 0]]\n",
    ").float()[None,None]\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can see that it focuses on the second key and returns the second value. \n",
    "\n",
    "If we give a query that matches two keys exactly, it should return the averaged value of the two values for those two keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01])\n",
      "Output is: tensor([549.9979,   5.5000,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor([[0, 0, 10]]).float()  \n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it focuses equally on the third and fourth key and returns the average of their values.\n",
    "\n",
    "Now giving all the queries at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are: tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
      "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
      "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
      "Output is: tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
      "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
      "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "test_Q = torch.tensor(\n",
    "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
    ").float()[None,None]\n",
    "print_out(test_Q, test_K, test_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D convolution with `kernel_size = 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically an MLP with one hidden layer and ReLU activation applied to each and every element in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.k1convL1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.k1convL1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.k1convL2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all components for our Transformer Encoder block shown below!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, conv_hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cnn = CNN(d_model, conv_hidden_dim)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Multi-head attention \n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        \n",
    "        # Layer norm after adding the residual connection \n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # Feed forward \n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        cnn_output = self.cnn(out1)\n",
    "        \n",
    "        # Second layer norm after adding residual connection \n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        out2 = self.layernorm2(out1 + cnn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder \n",
    "#### Blocks of N Encoder Layers + Positional encoding + Input embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention by itself does not have any recurrence or convolutions so to make it sensitive to position we must provide additional positional encodings. These are calculated as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "E(p, 2i)    &= \\sin(p / 10000^{2i / d}) \\\\\n",
    "E(p, 2i+1) &= \\cos(p / 10000^{2i / d})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
    "    theta = np.array([\n",
    "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "        for p in range(nb_p)\n",
    "    ])\n",
    "    E.requires_grad = False\n",
    "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
    "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
    "    E = E.to(device)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, max_position_embeddings):\n",
    "        super().__init__()\n",
    "        # vocab_size X d_model\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
    "        # max_position X d_model\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            max_position_embeddings, d_model)\n",
    "        create_sinusoidal_embeddings(\n",
    "            nb_p=max_position_embeddings,\n",
    "            dim=d_model,\n",
    "            E=self.position_embeddings.weight\n",
    "        )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        # (max_seq_length)\n",
    "        position_ids = torch.arange(\n",
    "            seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        # (bs, max_seq_length)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        # Get word embeddings for each input id\n",
    "        # (bs, max_seq_length, dim)\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # Get position embeddings for each position id\n",
    "        # (bs, max_seq_length, dim)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Add them both\n",
    "        # (bs, max_seq_length, dim)\n",
    "        embeddings = word_embeddings + position_embeddings\n",
    "        \n",
    "        # Layer norm\n",
    "        # (bs, max_seq_length, dim)\n",
    "        embeddings = self.LayerNorm(embeddings) \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, num_layers, d_model, num_heads, ff_hidden_dim,\n",
    "            input_vocab_size, maximum_position_encoding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embeddings(\n",
    "            d_model, input_vocab_size, maximum_position_encoding)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.enc_layers.append(\n",
    "                EncoderLayer(d_model, num_heads, ff_hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transform to (batch_size, input_seq_length, d_model)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        # (batch_size, input_seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import partial\n",
    "from torchtext.datasets import IMDB, NUM_LINES\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_words = 20_000\n",
    "num_test = NUM_LINES['IMDB']['test']\n",
    "split_num =  num_test // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "ds_train_int, ds_test_int = IMDB(root='./', split=('train', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train_int.shuffle(buffer_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Zero Run: 0 Max One Run: 0 Other: 1250\n",
      "Mean batch sentiment: 0.4996631578947368\n"
     ]
    }
   ],
   "source": [
    "sentiments = []\n",
    "zero = 0\n",
    "one = 0\n",
    "other = 0\n",
    "max_zero_run = 0\n",
    "max_one_run = 0\n",
    "zero_run = 0\n",
    "one_run = 0\n",
    "\n",
    "it_ = iter(ds_train)\n",
    "batch_size = 20\n",
    "train_len = NUM_LINES['IMDB']['train']\n",
    "i = 0\n",
    "batch = []\n",
    "\n",
    "for x, _ in it_:\n",
    "    i += 1\n",
    "    if i < 20:\n",
    "        batch.append(0 if x  == 'neg' else 1)\n",
    "        continue\n",
    "    avg = np.mean(batch)\n",
    "    if not (avg >= 0 or avg <= 1):\n",
    "        print(f'i: {i} batch: {batch}')\n",
    "        break\n",
    "    if avg == 0:\n",
    "        zero += 1\n",
    "        zero_run += 1\n",
    "        # if one_run != 0:\n",
    "        #     print(f'One run: {one_run}')\n",
    "        if one_run > max_one_run:\n",
    "            max_one_run = one_run\n",
    "        one_run = 0\n",
    "    elif avg == 1:\n",
    "        one += 1\n",
    "        one_run += 1\n",
    "        # if zero_run != 0:\n",
    "        #     print(f'Zero run: {zero_run}')\n",
    "        if zero_run > max_zero_run:\n",
    "            max_zero_run = zero_run\n",
    "        zero_run = 0\n",
    "    else:\n",
    "        other += 1\n",
    "    sentiments.append(avg)\n",
    "\n",
    "    i = 0\n",
    "    batch = []\n",
    "print(f'Max Zero Run: {max_zero_run} Max One Run: {max_one_run} Other: {other}')\n",
    "#print(f'Mean batch sentiment: {np.mean(sentiments)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekka/anaconda3/envs/pDL/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/utils.py:42: UserWarning: The input iterable can not be deepcopied, please be aware of in-place modification would affect source data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 8, 5, 0, 6, 7, 2, 4, 9]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "\n",
    "def my_ints():\n",
    "    for i in range(10):\n",
    "        yield i\n",
    "dp = IterableWrapper(my_ints())\n",
    "shuffle_dp = dp.shuffle()\n",
    "list(shuffle_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "    def __init__(self, total : int, num_classes : int):\n",
    "        self.count = 0\n",
    "        self.total = total\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def __call__(self, inst_id : int, elem : Tuple) -> int:\n",
    "        _ = elem\n",
    "        ret = (self.count % self.num_classes) == inst_id\n",
    "        self.count += 1\n",
    "        return int(ret)\n",
    "\n",
    "splitter = Splitter(num_test, 2)\n",
    "ds_valid, ds_test = ds_test_int.demux(\n",
    "    num_instances=2, classifier_fn=partial(splitter, 1), buffer_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.datapipes.iter.combining._ChildDataPipe"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "for _, line in ds_train:\n",
    "    counter.update(tokenizer(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woulda', 12)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common()[num_words:num_words+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(stream_iter):\n",
    "    for _, line in stream_iter:\n",
    "        yield tokenizer(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_obj = build_vocab_from_iterator(\n",
    "    yield_tokens(stream_iter=ds_train), max_tokens=num_words,\n",
    "    specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_obj.set_default_index(vocab_obj[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the new vocab is 20000\n",
      "The token at index 2 is <bos>\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of the new vocab is\", len(vocab_obj))\n",
    "new_stoi = vocab_obj.get_stoi()\n",
    "#print(\"The index of '' is\", new_stoi[''])\n",
    "new_itos = vocab_obj.get_itos()\n",
    "print(\"The token at index 2 is\", new_itos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_IDX = vocab_obj['<bos>']\n",
    "EOS_IDX = vocab_obj['<eos>']\n",
    "PAD_IDX = vocab_obj['<pad>']\n",
    "UNK_IDX = vocab_obj['<unk>']\n",
    "\n",
    "#train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
    "#    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transform(vocab, line):\n",
    "    return [BOS_IDX] + [vocab[tok] for tok in tokenizer(line)] + [EOS_IDX]\n",
    "\n",
    "def label_transform(label):\n",
    "    return 1 if label == 'pos' else 0\n",
    "\n",
    "def collate_batch(vocab, batch):\n",
    "    label_list, text_list = [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(torch.tensor(label_transform(_label)))\n",
    "        text_list.append(torch.tensor(text_transform(vocab, _text)))\n",
    "    text_list = pad_sequence(text_list, padding_value=PAD_IDX, batch_first=True)\n",
    "    label_list = torch.tensor(\n",
    "        label_list).unsqueeze(1).type(\n",
    "        torch.FloatTensor)\n",
    "    return text_list.to(device), label_list.to(device)\n",
    "\n",
    "collate_batch_vocab = partial(collate_batch, vocab_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    ds_train, batch_size=batch_size, shuffle=None,\n",
    "    collate_fn=collate_batch_vocab)\n",
    "valid_loader = DataLoader(\n",
    "    ds_valid, batch_size=batch_size, shuffle=False,\n",
    "    collate_fn=collate_batch_vocab)\n",
    "test_loader = DataLoader(\n",
    "    ds_test, batch_size=batch_size, shuffle=False,\n",
    "    collate_fn=collate_batch_vocab)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Zero Run: 0 Max One Run: 0 Other: 1250\n"
     ]
    }
   ],
   "source": [
    "sentiments = []\n",
    "zero = 0\n",
    "one = 0\n",
    "other = 0\n",
    "max_zero_run = 0\n",
    "max_one_run = 0\n",
    "zero_run = 0\n",
    "one_run = 0\n",
    "\n",
    "# next(iter(valid_loader))\n",
    "for _, y in train_loader:\n",
    "    avg = y.cpu().numpy().mean()\n",
    "    if avg == 0:\n",
    "        zero += 1\n",
    "        zero_run += 1\n",
    "        # if one_run != 0:\n",
    "        #     print(f'One run: {one_run}')\n",
    "        if one_run > max_one_run:\n",
    "            max_one_run = one_run\n",
    "        one_run = 0\n",
    "    elif avg == 1:\n",
    "        one += 1\n",
    "        one_run += 1\n",
    "        # if zero_run != 0:\n",
    "        #     print(f'Zero run: {zero_run}')\n",
    "        if zero_run > max_zero_run:\n",
    "            max_zero_run = zero_run\n",
    "        zero_run = 0\n",
    "    else:\n",
    "        other += 1\n",
    "    sentiments.append(avg)\n",
    "print(f'Max Zero Run: {max_zero_run} Max One Run: {max_one_run} Other: {other}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_batches(loader):\n",
    "    count = 0\n",
    "    for _ in loader:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated training batches: 1250 actual training batches 1250\n"
     ]
    }
   ],
   "source": [
    "est_train_batches = (NUM_LINES['IMDB']['train'] + (batch_size-1)) // batch_size\n",
    "# train_batches = count_batches(train_loader)\n",
    "train_batches = est_train_batches\n",
    "print(f'Estimated training batches: {est_train_batches} '\n",
    "    f'actual training batches {train_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated test batches: 625 actual test batches 625\n"
     ]
    }
   ],
   "source": [
    "est_test_batches = ((NUM_LINES['IMDB']['test'] // 2) + (batch_size-1)) \\\n",
    "    // batch_size\n",
    "#test_batches = count_batches(test_loader)\n",
    "test_batches = est_test_batches\n",
    "print(f'Estimated test batches: {est_test_batches} '\n",
    "    f'actual test batches {test_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated valid batches: 625 actual valid batches 625\n"
     ]
    }
   ],
   "source": [
    "est_valid_batches = ((NUM_LINES['IMDB']['test'] // 2) + (batch_size-1)) \\\n",
    "    // batch_size\n",
    "# valid_batches = count_batches(valid_loader)\n",
    "valid_batches = est_valid_batches\n",
    "print(f'Estimated valid batches: {est_valid_batches} '\n",
    "    f'actual valid batches {valid_batches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self, num_layers, d_model, num_heads, conv_hidden_dim,\n",
    "            input_vocab_size, num_answers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size,\n",
    "            maximum_position_encoding=10000)\n",
    "        self.dense = nn.Linear(d_model, num_answers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x, _ = torch.max(x, dim=1)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(\n",
    "    num_layers=1, d_model=32, num_heads=2, conv_hidden_dim=128,\n",
    "    input_vocab_size=num_words, num_answers=1)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, nb_batches):\n",
    "    data_iterator = iter(data_loader)\n",
    "    model.eval()\n",
    "    acc = 0\n",
    "    for x, y in data_iterator:\n",
    "        out = model(x)\n",
    "        acc += ((torch.sigmoid(out)>0.5) == y).cpu().numpy().mean()\n",
    "\n",
    "    print(f\"Eval accuracy: {acc / nb_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_loader, nb_batches_train,\n",
    "    valid_loader, nb_batches_valid):\n",
    "    for epoch in range(epochs):\n",
    "        train_iterator = iter(train_loader)\n",
    "        train_acc = 0\n",
    "        model.train()\n",
    "        losses = 0.0\n",
    "        count = 0\n",
    "        for x, y in train_iterator:\n",
    "            count += 1\n",
    "            out = model(x)  # ①\n",
    "            loss = criterion(out, y)  # ②\n",
    "            model.zero_grad()  # ③\n",
    "            loss.backward()  # ④\n",
    "            losses += loss.item()\n",
    "            optimizer.step()  # ⑤\n",
    "            acc = ((torch.sigmoid(out) > 0.5) == y).cpu().numpy().mean()\n",
    "            # pos_perc = y.cpu().numpy().mean()\n",
    "            # print(f'Iteration {count}: accuracy: {acc} loss: {loss.item()} '\n",
    "            #     f'Percent Positive: {pos_perc}')\n",
    "            train_acc += acc\n",
    "\n",
    "        \n",
    "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
    "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
    "        print('Evaluating on validation:')\n",
    "        evaluate(valid_loader, nb_batches_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 0 is 0.4321249868035317\n",
      "Training accuracy: 0.8009599999999995\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8062400000000002\n",
      "Training loss at epoch 1 is 0.33005270760655403\n",
      "Training accuracy: 0.8579599999999998\n",
      "Evaluating on validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekka/anaconda3/envs/pDL/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:248: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy: 0.8219199999999999\n",
      "Training loss at epoch 2 is 0.26211003826856616\n",
      "Training accuracy: 0.8938000000000027\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8419199999999993\n",
      "Training loss at epoch 3 is 0.2108477133527398\n",
      "Training accuracy: 0.9172400000000058\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.84304\n",
      "Training loss at epoch 4 is 0.16937405742555856\n",
      "Training accuracy: 0.9364800000000066\n",
      "Evaluating on validation:\n",
      "Eval accuracy: 0.8284799999999981\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, train_batches, valid_loader, valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy: 0.8292799999999972\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_loader, test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> I took this out <unk> from the library the other night , having no idea of the film 's cult , influence , or that it is currently being staged as a musical . ( ! ) Most of the comments here are on target , it 's moving , funny , sad , and yes , a tad exploitive despite the best intentions of the filmmakers . The expanded <unk> edition is a must for anyone who loved it when it came out . < br /><br />I think you can also see in little Edie the fall of a class that sort of disappeared , you can hear it in old films of Jackie O too ; people just do n't talk like that anymore . I think as a documentary , it would have been interesting to get more information about how the home fell into <unk> , Old Edie at least still seems aware of what 's going on to a certain degree ; could n't She see the once spectacular home <unk> ? < br /><br />Yet the film 's subject is the life the two women have constructed for themselves now , a real life <unk> Williams one act . Well worth your time . <eos>\n",
      "\n",
      "Review: tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "it_ = iter(train_loader)\n",
    "it_ = islice(it_, idx, idx+1)\n",
    "items = next(it_)\n",
    "toks = [t for t in items[0][0] if t != PAD_IDX]\n",
    "toks = vocab_obj.lookup_tokens(toks)\n",
    "print(' '.join(toks))\n",
    "print(f'\\nReview: {items[1][0]}')\n",
    "# review_sum = sum([i for _, y in iter(valid_loader) for i in y])\n",
    "# print(f'Number positive reviews : {review_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.1716e-01, 1.0000e+00],\n",
      "        [1.3316e-01, 0.0000e+00],\n",
      "        [6.8777e-02, 0.0000e+00],\n",
      "        [9.9864e-01, 1.0000e+00],\n",
      "        [3.9816e-04, 0.0000e+00],\n",
      "        [9.9287e-01, 1.0000e+00],\n",
      "        [3.7836e-02, 0.0000e+00],\n",
      "        [1.0492e-03, 0.0000e+00],\n",
      "        [5.2196e-01, 1.0000e+00],\n",
      "        [9.9052e-01, 1.0000e+00],\n",
      "        [5.6175e-04, 0.0000e+00],\n",
      "        [5.9922e-03, 0.0000e+00],\n",
      "        [9.8571e-01, 1.0000e+00],\n",
      "        [3.1491e-02, 0.0000e+00],\n",
      "        [3.3671e-02, 0.0000e+00],\n",
      "        [1.9651e-02, 0.0000e+00],\n",
      "        [4.0941e-01, 0.0000e+00],\n",
      "        [9.9138e-01, 1.0000e+00],\n",
      "        [5.5386e-01, 1.0000e+00],\n",
      "        [6.6777e-01, 1.0000e+00]], device='cuda:0')\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# items = next(iter(train_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(items[0])\n",
    "    sig_out = torch.sigmoid(out)\n",
    "    print(torch.cat((sig_out, items[1]), dim=1))\n",
    "    print(((sig_out > 0.5) == items[1]).cpu().numpy().mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pDL': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "730b8a846e5fbc1c9bd432ae5f8035781806fbee589b6661e19f1ae991cfab52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
